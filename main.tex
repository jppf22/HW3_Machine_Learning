\documentclass[12pt]{article}
\usepackage[paper=letterpaper,margin=1.5cm]{geometry}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{newtxtext, newtxmath}
\usepackage{enumitem}
\usepackage{titling}
\usepackage[colorlinks=true]{hyperref}

\usepackage{listings}
\usepackage[font=small,labelfont=bf]{caption} % Required for specifying captions to tables and figures

\usepackage{graphicx} % Required for the inclusion of images
\graphicspath{{./images/}} % Specifies the directory where pictures are stored

\setlength{\droptitle}{-6em}

\begin{document}

\center
Aprendizagem 2024\\
Homework I -- Group 016\\
(ist1106022, ist1106720)\vskip 1cm

\large{\textbf{Part I}: Pen and paper}\normalsize

\large{\textbf{Part II}: Programming}\normalsize

\begin{enumerate}[leftmargin=\labelsep, label=\textbf{\arabic*.)}]
    \item To compare the performance of a \textit{kNN} classifier with $k=5$ and a naive Bayes classifier, a 5-fold stratified cross-validation was performed on the on a heart disease dataset:

          \begin{enumerate}[label=\textbf{\alph*.)}]
              \item For comparing accuracies of the two classifiers, a box plot was generated for each:


                    The performance of the \textit{kNN} is less consistent as the box plot is wider, while the Naive Bayes classifier has a more consistent performance, as the box plot is narrower.

                    This is due to the fact that the \textit{kNN} is non-parametric and therefore is more sensitive across folds, and with non-normalized data, different scaled features, redudant, irrelevant or noisy features can affect the performance of the classifier. 

                    The Naive Bayes classifier, on the other hand, is parametric and is not sensitive to these factors.

              \item However, when choosing to scale the feature data with a Min-Max Scaler, the box plots look as follows:



                    This happens because the \textit{kNN} classifier, when choosing to normalize feature data, it adresses the previously mentioned problems and leads to heavily reduced variability, which in turn leads to a more consistent and accurate classification.

                    For \textit{Naive Bayes}, the scaling of the data does not generally affect the performance, since it is a parametric model.

              \item Perfoming a paired t-test on the accuracies of the two classifiers, using \texttt{scipy}'s method \texttt{ttest\_rel} that considers $H_0$ to be the hypothesis that the two classifiers have the same stastical significance regarding accuracy, and $H_1$ to be the hypothesis that \textit{kNN} is more accurate than Naive Bayes, the results are as follows:

                    \begin{itemize}
                        \item When not scaling feature data: $P\text{-value} = 0.998415501126768$
                              \begin{itemize}
                                  \item Considering a $1\%$ threshold, \textit{kNN} is not statistically superior than Bayes
                                  \item Considering a $5\%$ threshold, \textit{kNN} is not statistically superior than Bayes
                                  \item Considering a $10\%$ threshold, \textit{kNN} is not statistically superior than Bayes
                              \end{itemize}
                        \item When min-max scaling feature data: $P\text{-value} = 0.7532332545792753$
                              \begin{itemize}
                                  \item Considering a $1\%$ threshold, \textit{kNN} is not statistically superior than Bayes
                                  \item Considering a $5\%$ threshold, \textit{kNN} is not statistically superior than Bayes
                                  \item Considering a $10\%$ threshold, \textit{kNN} is not statistically superior than Bayes
                              \end{itemize}
                    \end{itemize}

                    We can therefore easily conclude that the hypothesis "the \textit{kNN} model is statistically superior to
                    na√Øve Bayes regarding accuracy" is \textbf{false / rejected}.
          \end{enumerate}
    \item To compare the performance of uniform and distance-based weights \textit{kNN} classifiers with varying amounts of neighbors (\textit{k}) used in classifications, a 80-20 train-test split was performed for each combination:

          \begin{enumerate}[label=\textbf{\alph*.)}]
              \item The following plots showcase the obtained results:



              \item Generally, the bigger the value of \textit{k} on a \textit{kNN} classifier, the more accurate the predictions are, until a certain point. This point, for either uniform or distance-based weights, is when the accuracies on both training and testing data is the highest, which seems to be around $k = 30$ for this dataset.


                    In terms of generalization capabilities, the distance-based weighting system seems to be more overfitted, with the training accuracy being unchanged and the testing accuracy flatlining. The uniform weighting system, however, seems to be more general, with the training and testing accuracy approaching each other as the number of neighbors increases.

                    Before the suggested point, for the uniform weights, the model is overfitting on the training data.
                    Past $k=30$ it may be at the risk of underfitting, a phenomenon not observable for the dataset at hand.

          \end{enumerate}

    \item Some properties from the dataset that may justify the shortcomings of Naive Bayes classifier on this dataset are:
          \begin{itemize}
              \item Since Naive Bayes assumes that all features are independent, it ignores the relantionships and correlation that these features may have in relation to heart disease diagnosis.
              \item The dataset has features which are not normally distributed or are not numerical, which is a violation of the Gaussian Naive Bayes conditions. Some of these features include the categorical features, such as $cp$, $fbs$, $resteg$, $exang$ and $slope$, for example. This leads to incorrect probability calculation, and therefore, incorrect classification.
          \end{itemize}

\end{enumerate}
\end{document}
